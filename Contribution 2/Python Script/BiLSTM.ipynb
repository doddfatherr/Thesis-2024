{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTA9EyEMZ-DN"
      },
      "source": [
        "# BiLSTM Model\n",
        "\n",
        "This script allows for the training of a Bidirectional Long-Short-Term Memory (BiLSTM) recurrent neural network. The script:\n",
        "\n",
        "*   Takes .txt files as input (formatted similarly to Ninapro .txt files)\n",
        "*   Preprocesses the data, generates the model input and pickles the input data\n",
        "*   Trains and evaluates the model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Model architecture and functions from existing GitHub Repo:\n",
        "https://github.com/sherry-s-yuan/gesture_classification/tree/master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUo6rXdTs7ED"
      },
      "source": [
        "# Initilaize Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRWXxjSqm9yP",
        "outputId": "b1c7b25e-d178-4ec1-9e2f-ba5823e2dc17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Input, Flatten, Dense, Dropout, LSTM, Bidirectional, concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score, f1_score, recall_score\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# Mount google Drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozh889-soNvJ"
      },
      "source": [
        "# BiLSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gAphq1oenI1L"
      },
      "outputs": [],
      "source": [
        "class BILSTM:\n",
        "    def train(self, X_emg_train, X_acc_train, zc_features_train, ssc_features_train, features_1d_train_emg, features_1d_train_acc, Y_train,\n",
        "              X_emg_val, X_acc_val, zc_features_val, ssc_features_val, features_1d_val_emg, features_1d_val_acc, Y_val, num_activation=100):\n",
        "\n",
        "        \"\"\"\"construct model architecture\"\"\"\n",
        "        # input layer setup\n",
        "        x_emg_branch_input = Input(shape=(X_emg_train.shape[1], X_emg_train.shape[2]))\n",
        "        x_acc_branch_input = Input(shape=(X_acc_train.shape[1], X_acc_train.shape[2]))\n",
        "        zc_branch_input = Input(shape=(zc_features_train.shape[1], zc_features_train.shape[2]))\n",
        "        ssc_branch_input = Input(shape=(ssc_features_train.shape[1], ssc_features_train.shape[2]))\n",
        "        oned_branch_input_emg = Input(shape=(features_1d_train_emg.shape[1], features_1d_train_emg.shape[2]))\n",
        "        oned_branch_input_acc = Input(shape=(features_1d_train_acc.shape[1], features_1d_train_acc.shape[2]))\n",
        "\n",
        "        # branch setup\n",
        "        x_emg_branch = Bidirectional(LSTM(num_activation))(x_emg_branch_input)\n",
        "        x_emg_branch = Dropout(0.5)(x_emg_branch)\n",
        "        x_emg_branch = Dense(num_activation, activation='relu')(x_emg_branch)\n",
        "        x_emg_branch = Model(inputs=x_emg_branch_input, outputs=x_emg_branch)\n",
        "\n",
        "        x_acc_branch = Bidirectional(LSTM(num_activation))(x_acc_branch_input)\n",
        "        x_acc_branch = Dropout(0.5)(x_acc_branch)\n",
        "        x_acc_branch = Dense(num_activation, activation='relu')(x_acc_branch)\n",
        "        x_acc_branch = Model(inputs=x_acc_branch_input, outputs=x_acc_branch)\n",
        "\n",
        "        zc_branch = Bidirectional(LSTM(num_activation))(zc_branch_input)\n",
        "        zc_branch = Dropout(0.3)(zc_branch)\n",
        "        zc_branch = Dense(num_activation, activation='relu')(zc_branch)\n",
        "        zc_branch = Model(inputs=zc_branch_input, outputs=zc_branch)\n",
        "\n",
        "        ssc_branch = Bidirectional(LSTM(num_activation))(ssc_branch_input)\n",
        "        ssc_branch = Dropout(0.3)(ssc_branch)\n",
        "        ssc_branch = Dense(num_activation, activation='relu')(ssc_branch)\n",
        "        ssc_branch = Model(inputs=ssc_branch_input, outputs=ssc_branch)\n",
        "\n",
        "        oned_branch_emg = Flatten()(oned_branch_input_emg)\n",
        "        oned_branch_emg = Dense(num_activation, activation='relu')(oned_branch_emg)\n",
        "        oned_branch_emg = Model(inputs=oned_branch_input_emg, outputs=oned_branch_emg)\n",
        "\n",
        "        oned_branch_acc = Flatten()(oned_branch_input_acc)\n",
        "        oned_branch_acc = Dense(num_activation, activation='relu')(oned_branch_acc)\n",
        "        oned_branch_acc = Model(inputs=oned_branch_input_acc, outputs=oned_branch_acc)\n",
        "\n",
        "\n",
        "        # root setup\n",
        "        combined = concatenate([x_emg_branch.output, x_acc_branch.output, zc_branch.output, ssc_branch.output, oned_branch_emg.output, oned_branch_acc.output])\n",
        "        combined = Dense(num_activation, activation='relu')(combined)\n",
        "        combined = Dropout(0.3)(combined)\n",
        "        combined = Dense(7, activation='relu')(combined)\n",
        "        combined = Dense(2, activation='softmax')(combined)\n",
        "\n",
        "        # compile model\n",
        "        self.model = Model(inputs=[x_emg_branch_input, x_acc_branch_input, zc_branch_input, ssc_branch_input, oned_branch_input_emg, oned_branch_input_acc], outputs=combined)\n",
        "\n",
        "        # Create optimizer with custom learning rate schedule\n",
        "        initial_learning_rate = 0.01\n",
        "        decay_factor = 0.1\n",
        "        decay_steps = 1000\n",
        "        lr_schedule = CustomLearningRateSchedule(initial_learning_rate, decay_factor, decay_steps)\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        # Calculate class weights\n",
        "        # Flatten Y_train and Y_val\n",
        "        Y_train_flat = np.argmax(Y_train, axis=1)\n",
        "        Y_val_flat = np.argmax(Y_val, axis=1)\n",
        "        class_labels = np.unique(Y_train)\n",
        "        class_weights = compute_class_weight(\n",
        "                                                  class_weight = 'balanced',\n",
        "                                                  classes = class_labels,\n",
        "                                                  y=Y_train_flat)\n",
        "\n",
        "        # Convert class weights to a dictionary\n",
        "        class_weight_dict = dict(zip(class_labels, class_weights))\n",
        "\n",
        "        optimize = keras.optimizers.Adam(learning_rate=0.001)\n",
        "        # Compile model with class weights\n",
        "        self.model.compile(loss=\"categorical_crossentropy\", optimizer=optimize, metrics=['accuracy'])\n",
        "\n",
        "        # fit model\n",
        "        history = self.model.fit([X_emg_train, X_acc_train, zc_features_train, ssc_features_train, features_1d_train_emg, features_1d_train_acc], Y_train, epochs=60, batch_size=256,\n",
        "                        validation_data=([X_emg_val, X_acc_val, zc_features_val, ssc_features_val, features_1d_val_emg, features_1d_val_acc], Y_val), class_weight=class_weight_dict)\n",
        "\n",
        "        print(self.model.summary())\n",
        "\n",
        "        # Plot training history (optional)\n",
        "        self.plot_training_history(history)\n",
        "\n",
        "    def plot_training_history(self, history):\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Plot training & validation accuracy values\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title('Model accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot training & validation loss values\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Model loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    # Model Evaluation\n",
        "    def evaluate(self, X_emg_test, X_acc_test, zc_features_test, ssc_features_test, features_1d_test_emg, features_1d_test_acc, Y_test):\n",
        "        loss, accuracy = self.model.evaluate([X_emg_test, X_acc_test, zc_features_test, ssc_features_test, features_1d_test_emg, features_1d_test_acc], Y_test)\n",
        "        predictions = self.model.predict([X_emg_test, X_acc_test, zc_features_test, ssc_features_test, features_1d_test_emg, features_1d_test_acc])\n",
        "        # Calculate Metrics\n",
        "        precision = precision_score(np.argmax(Y_test, axis=1), np.argmax(predictions, axis=1))\n",
        "        recall = recall_score(np.argmax(Y_test, axis=1), np.argmax(predictions, axis=1))\n",
        "        f1 = f1_score(np.argmax(Y_test, axis=1), np.argmax(predictions, axis=1))\n",
        "        print(\"Test Loss:\", loss)\n",
        "        print(\"Test Accuracy:\", accuracy)\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"F1 Score:\", f1)\n",
        "        print(\"Recall:\", recall)\n",
        "        return accuracy, loss, f1, precision, recall\n",
        "\n",
        "    def predict(self, X_emg, zc_features, ssc_features, features_1d_emg, features_1d_acc):\n",
        "        # precondition: passed in data must be normalized.\n",
        "        return self.get_class(self.model.predict([X_emg, X_acc, zc_features, ssc_features, features_1d_emg, features_1d_acc]))\n",
        "\n",
        "    def get_class(self, predictions):\n",
        "      \"\"\"get class label from one hot encoding\"\"\"\n",
        "      if isinstance(predictions, int):\n",
        "        # IF predictions is a single integer, return it directly\n",
        "        return predictions\n",
        "      elif isinstance(predictions, np.ndarray):\n",
        "        result = np.argmax(predictions, axis = 1)\n",
        "        return result\n",
        "      else:\n",
        "          #prediction = prediction.tolist()\n",
        "          raise ValueError(\"Unsupported type for prediction: {}\".format(type(predictions)))\n",
        "        #return np.array(result)\n",
        "\n",
        "    def load_model(file_path):\n",
        "        smodel = load_model('file_path')\n",
        "\n",
        "    def save_model(self):\n",
        "        self.model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqyYBj2QjHI2"
      },
      "source": [
        "Learning rate decay function (did not end up using)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S5kju2xvjGtV"
      },
      "outputs": [],
      "source": [
        "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_learning_rate, decay_factor, decay_steps):\n",
        "        super(CustomLearningRateSchedule, self).__init__()\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.decay_factor = decay_factor\n",
        "        self.decay_steps = decay_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        return self.initial_learning_rate * self.decay_factor ** (step / self.decay_steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBtBLuqRo1_k"
      },
      "source": [
        "# Preprocessing Data b4 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y2h6gJK6o1NG"
      },
      "outputs": [],
      "source": [
        "class Signal:\n",
        "    def __init__(self, path):\n",
        "        self.data = pd.read_csv(path, delimiter='\\t', encoding='UTF-8')\n",
        "        # filter out class 0 and 7\n",
        "        self.data = self.data.loc[self.data['class'] != 7]\n",
        "        #self.data = self.data.loc[self.data['class'] != 0]\n",
        "        # load in data\n",
        "        self.label = self.data['class'].to_numpy()\n",
        "        self.time = self.data['time']\n",
        "        self.data_emg = self.data.iloc[:, 1:9]\n",
        "        self.data_acc = self.data.iloc[:, 9:12]\n",
        "        self.x_emg = []\n",
        "        self.x_acc = []\n",
        "        self.filename = path.split('/')[-1].split('.txt')[0]\n",
        "\n",
        "    # Window Raw Data\n",
        "    def generate(self, step=20, window=375):\n",
        "        \"\"\"break time series data into periods of windows of 0.75 seconds for detection\"\"\"\n",
        "        self.x_emg = []\n",
        "        self.x_acc = []\n",
        "        self.y = []\n",
        "        print('Generating Data...')\n",
        "        for i in range(0, self.data_emg.shape[0], step):\n",
        "            if self.data_emg.shape[0] - i < 375:\n",
        "                break\n",
        "            # Generate one hot encoding of labels\n",
        "            label = [0, 0]  # Two classes\n",
        "            majority_class = self._majority_class(self.label[i:i+window])\n",
        "            label[majority_class] = 1\n",
        "            self.y.append(label)\n",
        "            self.x_emg.append(self.data_emg.iloc[i:i+window, :])\n",
        "            self.x_acc.append(self.data_acc.iloc[i:i+window, :])\n",
        "        self.x_emg = np.array(self.x_emg)\n",
        "        self.x_acc = np.array(self.x_acc)\n",
        "        self.y = np.array(self.y)\n",
        "\n",
        "    # Generate features\n",
        "    def feature_generation(self):\n",
        "        \"\"\"prepare all ssc, zc, average, rms etc features\"\"\"\n",
        "        self.ssc_feature = []\n",
        "        self.zc_feature = []\n",
        "        self.feature_1d_emg = []\n",
        "        self.feature_1d_acc = []\n",
        "        for timeframe in self.x_emg:\n",
        "            self.ssc_feature.append(self.SSC_EMG(timeframe))\n",
        "            self.zc_feature.append(self.ZC_EMG(timeframe))\n",
        "            self.feature_1d_emg.append(self.feature_extraction_1D_emg(timeframe))\n",
        "        for timeframe in self.x_acc:\n",
        "            self.feature_1d_acc.append(self.feature_extraction_1D_acc(timeframe))\n",
        "\n",
        "        self.ssc_feature = np.array(self.ssc_feature)\n",
        "        self.zc_feature = np.array(self.zc_feature)\n",
        "        self.feature_1d_emg = np.array(self.feature_1d_emg)\n",
        "        self.feature_1d_acc = np.array(self.feature_1d_acc)\n",
        "\n",
        "    # Multi-Dimension Features\n",
        "    def ZC_EMG(self, x):\n",
        "        \"\"\"compute zero crossing, return (window, channel)\"\"\"\n",
        "        a2 = np.sign(x[:,:8])\n",
        "        change2 = ((np.roll(a2, 1, axis=0) - a2) != 0).astype(int)\n",
        "        return change2\n",
        "\n",
        "    def ZC_ACCEL(self, x):\n",
        "        \"\"\"compute zero crossing, return (window, channel)\"\"\"\n",
        "        a2 = np.sign(x[:,-3:])\n",
        "        change2 = ((np.roll(a2, 1, axis=0) - a2) != 0).astype(int)\n",
        "        return change2\n",
        "\n",
        "    def SSC_EMG(self, x):\n",
        "        \"\"\"detect slope change, return (window-1, channel)\"\"\"\n",
        "        a = np.sign(np.diff(x[:,:8], axis=0))\n",
        "        change = ((np.roll(a, 1, axis=0) - a) != 0).astype(int)\n",
        "        return change\n",
        "\n",
        "    def SSC_ACCEL(self, x):\n",
        "        \"\"\"detect slope change, return (window-1, channel)\"\"\"\n",
        "        a = np.sign(np.diff(x[:,-3:], axis=0))\n",
        "        change = ((np.roll(a, 1, axis=0) - a) != 0).astype(int)\n",
        "        return change\n",
        "\n",
        "# Single-Dimension Features\n",
        "\n",
        "    # Mean Absolute Value\n",
        "    def MAV_EMG(self, x):\n",
        "        \"\"\"compute mean absolute value, return (1, channel)\"\"\"\n",
        "        return np.mean(np.abs(x), axis=0) #pd.DataFrame(x.abs().mean(axis=0)).T\n",
        "\n",
        "    def MAV_acceleration(self, x):\n",
        "        \"\"\"Compute mean absolute value for acceleration data, return (1, channel)\"\"\"\n",
        "        return np.mean(np.abs(x), axis=0)\n",
        "\n",
        "    # Waveform Length (WL)\n",
        "    def WL_EMG(self, x):\n",
        "        \"\"\"waveform length feature extraction, return (1, channel)\"\"\"\n",
        "        # columns = x.columns\n",
        "        a = np.sum(np.diff(x, axis=0), axis=0)\n",
        "        # change = pd.DataFrame(a).T\n",
        "        # change.columns = columns\n",
        "        return a #change\n",
        "\n",
        "    def WL_acceleration(self, x):\n",
        "        \"\"\"Waveform length feature extraction for acceleration data, return (1, channel)\"\"\"\n",
        "        a = np.sum(np.diff(x, axis=0), axis=0)\n",
        "        return a\n",
        "\n",
        "    # Standard Deviation (STD)\n",
        "    def STD_EMG(self, x):\n",
        "        \"\"\"standard deviation of the channels, return (1, channel)\"\"\"\n",
        "        return x.std(axis=0) # pd.DataFrame(x.std(axis=0)).T\n",
        "\n",
        "    def STD_acceleration(self, x):\n",
        "        \"\"\"Standard deviation of the channels for acceleration data, return (1, channel)\"\"\"\n",
        "        return x.std(axis=0)\n",
        "\n",
        "    # Root Mean Square (RMS)\n",
        "    def RMS_EMG(self, x):\n",
        "        \"\"\"Root mean squared, return (1, channel)\"\"\"\n",
        "        return ((x ** 2).sum(axis=0) / x.shape[0]) ** (1 / 2) # pd.DataFrame(((x ** 2).sum(axis=0) / x.shape[0]) ** (1 / 2)).T\n",
        "\n",
        "    def RMS_acceleration(self, x):\n",
        "        \"\"\"Root mean squared for acceleration data, return (1, channel)\"\"\"\n",
        "        return ((x ** 2).sum(axis=0) / x.shape[0]) ** (1 / 2)\n",
        "\n",
        "\n",
        "    def _majority_class(self, labels, threshold=0.25):\n",
        "        \"\"\"Return 1 if positive count is 25% or more of the total count, otherwise return 0.\"\"\"\n",
        "        # Check if positive proportion meets or exceeds the threshold\n",
        "        positive_proportion = np.mean(labels)\n",
        "        if positive_proportion >= threshold:\n",
        "            first_value = labels[0]\n",
        "            final_value = labels[-1]\n",
        "            # Check if it's entering, in the middle, or leaving a window\n",
        "            if first_value == 0 and final_value == 1:\n",
        "                return 1\n",
        "            elif first_value == 1 and final_value == 1:\n",
        "                return 1\n",
        "            elif first_value == 1 and final_value == 0:\n",
        "                # Check if it's leaving and if the proportion is less than 0.5\n",
        "                if positive_proportion <= 0.5:\n",
        "                    return 0\n",
        "                else:\n",
        "                    return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "    def feature_extraction_1D_emg(self, x):\n",
        "        \"\"\"join all 1D feature together\"\"\"\n",
        "        #EMG Feature Extraction\n",
        "        rms = [self.RMS_EMG(x)]\n",
        "        wl_emg = self.WL_EMG(x)\n",
        "        std_emg = self.STD_EMG(x)\n",
        "        mav_emg = self.MAV_EMG(x)\n",
        "        #Append all features together now\n",
        "        rms.append(wl_emg)\n",
        "        rms.append(std_emg)\n",
        "        rms.append(mav_emg)\n",
        "        return np.array(rms)\n",
        "\n",
        "    def feature_extraction_1D_acc(self, x):\n",
        "        \"\"\"join all 1D feature together\"\"\"\n",
        "        #WACCEL Feature Extraction\n",
        "        rms = [self.RMS_acceleration(x)]\n",
        "        wl_accel = self.WL_acceleration(x)\n",
        "        std_accel = self.STD_acceleration(x)\n",
        "        mav_accel = self.MAV_acceleration(x)\n",
        "\n",
        "        #Append all features together now\n",
        "        rms.append(wl_accel)\n",
        "        rms.append(std_accel)\n",
        "        rms.append(mav_accel)\n",
        "        return np.array(rms)\n",
        "\n",
        "    def to_numpy(self):\n",
        "        self.data = self.data.to_numpy()\n",
        "\n",
        "    def save_data(self):\n",
        "        directory = \"./data\"\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        np.save(\"./data/\" + self.filename + '_y', self.y)\n",
        "        np.save(\"./data/\" + self.filename + '_x_emg', self.x_emg)\n",
        "        np.save(\"./data/\" + self.filename + '_x_acc', self.x_acc)\n",
        "        np.save(\"./data/\" + self.filename + '_zc', self.zc_feature)\n",
        "        np.save(\"./data/\" + self.filename + '_ssc', self.ssc_feature)\n",
        "        np.save(\"./data/\" + self.filename + '_1d_emg', self.feature_1d_emg)\n",
        "        np.save(\"./data/\" + self.filename + '_1d_acc', self.feature_1d_acc)\n",
        "\n",
        "    def load_data(self):\n",
        "        y = np.load(\"./data/\" + self.filename + '_y' + \".npy\")\n",
        "        x_emg = np.load(\"./data/\" + self.filename + '_x_emg' + \".npy\")\n",
        "        x_acc = np.load(\"./data/\" + self.filename + '_x_acc' + \".npy\")\n",
        "        zc_feature = np.load(\"./data/\" + self.filename + '_zc' + \".npy\")\n",
        "        ssc_feature = np.load(\"./data/\" + self.filename + '_ssc' + \".npy\")\n",
        "        feature_1d_emg = np.load(\"./data/\" + self.filename + '_1d_emg' + \".npy\")\n",
        "        feature_1d_acc= np.load(\"./data/\" + self.filename + '_1d_acc' + \".npy\")\n",
        "        return x_emg, x_acc, zc_feature, ssc_feature, feature_1d_emg, feature_1d_acc, y\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, X_emg, X_acc, zc_features, ssc_features, features_1d_emg, features_1d_acc, Y):\n",
        "        print(\"Train val Split:\")\n",
        "        # get training dataset index\n",
        "        self.original_indices = np.arange(X_emg.shape[0])\n",
        "        # Manually select train/validation split below.\n",
        "        # If doing k-fold, X_emg.shape[0] * 0.99\n",
        "        # If else, X_emg.shape[0] * 0.80\n",
        "        train_ind = np.random.choice(self.original_indices, int(X_emg.shape[0] * 0.99), replace=False)\n",
        "        val_ind = np.setdiff1d(self.original_indices, train_ind)\n",
        "\n",
        "        # Shuffle indices for training and validation sets\n",
        "        self.train_indices = train_ind\n",
        "        self.val_indices = val_ind\n",
        "\n",
        "        # Training data\n",
        "        self.X_emg_train = X_emg[train_ind]\n",
        "        self.X_acc_train = X_acc[train_ind]\n",
        "        self.zc_features_train = zc_features[train_ind]\n",
        "        self.ssc_features_train = ssc_features[train_ind]\n",
        "        self.features_1d_train_emg = features_1d_emg[train_ind]\n",
        "        self.features_1d_train_acc = features_1d_acc[train_ind]\n",
        "        self.Y_train = Y[train_ind]\n",
        "\n",
        "        # Validation data\n",
        "        self.X_emg_val = X_emg[val_ind]\n",
        "        self.X_acc_val = X_acc[val_ind]\n",
        "        self.zc_features_val = zc_features[val_ind]\n",
        "        self.ssc_features_val = ssc_features[val_ind]\n",
        "        self.features_1d_val_emg = features_1d_emg[val_ind]\n",
        "        self.features_1d_val_acc = features_1d_acc[val_ind]\n",
        "        self.Y_val = Y[val_ind]\n",
        "        # normalization scaler storage\n",
        "        self.x_emg_scaler = {}\n",
        "        self.x_acc_scaler = {}\n",
        "        self.zc_scaler = {}\n",
        "        self.ssc_scaler = {}\n",
        "        self.oned_scaler_emg = {}\n",
        "        self.oned_scaler_acc = {}\n",
        "        self.normalize_train_val() # normalize data using StandardScaler\n",
        "\n",
        "    def normalize_train_val(self):\n",
        "        # using scaler fitted on training data on val data\n",
        "        if len(self.x_emg_scaler) == 0:\n",
        "            # fit transform training set\n",
        "            for i in range(self.X_emg_train.shape[1]):\n",
        "                self.x_emg_scaler[i] = StandardScaler()\n",
        "                self.X_emg_train[:, i, :] = self.x_emg_scaler[i].fit_transform(self.X_emg_train[:, i, :])\n",
        "            for i in range(self.X_acc_train.shape[1]):\n",
        "                self.x_acc_scaler[i] = StandardScaler()\n",
        "                self.X_acc_train[:, i, :] = self.x_acc_scaler[i].fit_transform(self.X_acc_train[:, i, :])\n",
        "            for i in range(self.zc_features_train.shape[1]):\n",
        "                self.zc_scaler[i] = StandardScaler()\n",
        "                self.zc_features_train[:, i, :] = self.zc_scaler[i].fit_transform(self.zc_features_train[:, i, :])\n",
        "            for i in range(self.ssc_features_train.shape[1]):\n",
        "                self.ssc_scaler[i] = StandardScaler()\n",
        "                self.ssc_features_train[:, i, :] = self.ssc_scaler[i].fit_transform(self.ssc_features_train[:, i, :])\n",
        "            for i in range(self.features_1d_train_emg.shape[1]):\n",
        "                self.oned_scaler_emg[i] = StandardScaler()\n",
        "                self.features_1d_train_emg[:, i, :] = self.oned_scaler_emg[i].fit_transform(self.features_1d_train_emg[:, i, :])\n",
        "            for i in range(self.features_1d_train_acc.shape[1]):\n",
        "                self.oned_scaler_acc[i] = StandardScaler()\n",
        "                self.features_1d_train_acc[:, i, :] = self.oned_scaler_acc[i].fit_transform(self.features_1d_train_acc[:, i, :])\n",
        "        # fit transform val\n",
        "        for i in range(self.X_emg_val.shape[1]):\n",
        "            self.X_emg_val[:, i, :] = self.x_emg_scaler[i].transform(self.X_emg_val[:, i, :])\n",
        "        for i in range(self.X_acc_val.shape[1]):\n",
        "            self.X_acc_val[:, i, :] = self.x_acc_scaler[i].transform(self.X_acc_val[:, i, :])\n",
        "        for i in range(self.zc_features_val.shape[1]):\n",
        "            self.zc_features_val[:, i, :] = self.zc_scaler[i].transform(self.zc_features_val[:, i, :])\n",
        "        for i in range(self.ssc_features_val.shape[1]):\n",
        "            self.ssc_features_val[:, i, :] = self.ssc_scaler[i].transform(self.ssc_features_val[:, i, :])\n",
        "        for i in range(self.features_1d_val_emg.shape[1]):\n",
        "            self.features_1d_val_emg[:, i, :] = self.oned_scaler_emg[i].transform(self.features_1d_val_emg[:, i, :])\n",
        "        for i in range(self.features_1d_val_acc.shape[1]):\n",
        "            self.features_1d_val_acc[:, i, :] = self.oned_scaler_acc[i].transform(self.features_1d_val_acc[:, i, :])\n",
        "\n",
        "    def normalize(self, X_emg, X_acc, zc_features, ssc_features, features_1d_emg, features_1d_acc):\n",
        "        \"\"\"this function manipute the variables directly: aliasing\"\"\"\n",
        "        for i in range(self.X_emg_val.shape[1]):\n",
        "            X_emg[:, i, :] = self.x_emg_scaler[i].transform(X_emg[:, i, :])\n",
        "        for i in range(self.X_acc_val.shape[1]):\n",
        "            X_acc[:, i, :] = self.x_acc_scaler[i].transform(X_acc[:, i, :])\n",
        "        for i in range(self.zc_features_val.shape[1]):\n",
        "            zc_features[:, i, :] = self.zc_scaler[i].transform(zc_features[:, i, :])\n",
        "        for i in range(self.ssc_features_val.shape[1]):\n",
        "            ssc_features[:, i, :] = self.ssc_scaler[i].transform(ssc_features[:, i, :])\n",
        "        for i in range(self.features_1d_val_emg.shape[1]):\n",
        "            features_1d_emg[:, i, :] = self.oned_scaler_emg[i].transform(features_1d_emg[:, i, :])\n",
        "        for i in range(self.features_1d_val_acc.shape[1]):\n",
        "            features_1d_acc[:, i, :] = self.oned_scaler_acc[i].transform(features_1d_acc[:, i, :])\n",
        "        return X_emg, X_acc, zc_features, ssc_features, features_1d_emg, features_1d_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Testing Data\n",
        "This section can be used for generating data for both k-fold and individual model training"
      ],
      "metadata": {
        "id": "u9dYdt8NREo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyLyblTzLwi_",
        "outputId": "891c8a4d-5fc0-4971-ac4f-835f0617f140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare Training/Validation Paths:\n",
            "Processing Training/Validation Signal Files:\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/Dish Washing 1.txt\n",
            "Generating Data...\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/Dish Washing 2.txt\n",
            "Generating Data...\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/Dish Washing 3.txt\n",
            "Generating Data...\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/Dish Washing 4.txt\n",
            "Generating Data...\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/Dish Washing 5.txt\n",
            "Generating Data...\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/6_11_21_48.txt\n",
            "Generating Data...\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/7_11_28_40.txt\n",
            "Generating Data...\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing/8_11_36_05.txt\n",
            "Generating Data...\n",
            "Concatenate Training, Validation Data:\n",
            "Train val Split:\n",
            "Pickling Data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Prepare paths to data access\n",
        "print(\"Prepare Training/Validation Paths:\")\n",
        "paths = []\n",
        "repository = \"/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/DishWashing\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(repository):\n",
        "    # Get all .txt files in the directory\n",
        "    files = [os.path.join(repository, f) for f in os.listdir(repository) if f.endswith('.txt')]\n",
        "    paths.extend(files)\n",
        "else:\n",
        "    print(\"Directory does not exist:\", repository)\n",
        "\n",
        "# Preprocessing Signal File\n",
        "print(\"Processing Training/Validation Signal Files:\")\n",
        "signals = []\n",
        "count = 0\n",
        "for path in paths:\n",
        "    # if count >= 14:\n",
        "    #     break\n",
        "    print(path)\n",
        "    signal = Signal(path)\n",
        "    signal.to_numpy()\n",
        "    signal.generate()\n",
        "    signal.feature_generation()\n",
        "    signal.save_data()\n",
        "    signals.append(signal)\n",
        "    count += 1\n",
        "\n",
        "# Concatenate Train val Data\n",
        "print(\"Concatenate Training, Validation Data:\")\n",
        "X_emg, X_acc, zc_features, ssc_features, features_1d_emg, features_1d_acc, Y = signals[0].load_data()\n",
        "for signal in signals[1:]:\n",
        "    x_emg,x_acc, zc_feature, ssc_feature, feature_1d_emg, feature_1d_acc, y = signal.load_data()\n",
        "    X_emg = np.append(X_emg, x_emg, axis=0)\n",
        "    X_acc = np.append(X_acc, x_acc, axis=0)\n",
        "    zc_features = np.append(zc_features, zc_feature, axis=0)\n",
        "    ssc_features = np.append(ssc_features, ssc_feature, axis=0)\n",
        "    features_1d_emg = np.append(features_1d_emg, feature_1d_emg, axis=0)\n",
        "    features_1d_acc = np.append(features_1d_acc, feature_1d_acc, axis=0)\n",
        "    Y = np.append(Y, y, axis=0)\n",
        "\n",
        "# Check for NaN values in concatenated arrays\n",
        "nan_indices_X_emg = np.argwhere(np.isnan(X_emg))\n",
        "nan_indices_X_acc = np.argwhere(np.isnan(X_acc))\n",
        "nan_indices_zc = np.argwhere(np.isnan(zc_features))\n",
        "nan_indices_ssc = np.argwhere(np.isnan(ssc_features))\n",
        "nan_indices_features_1d_emg = np.argwhere(np.isnan(features_1d_emg))\n",
        "nan_indices_features_1d_acc = np.argwhere(np.isnan(features_1d_acc))\n",
        "nan_indices_Y = np.argwhere(np.isnan(Y))\n",
        "if len(nan_indices_X_emg) > 0:\n",
        "    print(\"NaN values found in X at indices:\", nan_indices_X_emg)\n",
        "if len(nan_indices_X_acc) > 0:\n",
        "    print(\"NaN values found in X at indices:\", nan_indices_X_acc)\n",
        "if len(nan_indices_zc) > 0:\n",
        "    print(\"NaN values found in zc_features at indices:\", nan_indices_zc)\n",
        "if len(nan_indices_ssc) > 0:\n",
        "    print(\"NaN values found in ssc_features at indices:\", nan_indices_ssc)\n",
        "if len(nan_indices_features_1d_emg) > 0:\n",
        "    print(\"NaN values found in features_1d at indices:\", nan_indices_features_1d_emg)\n",
        "if len(nan_indices_features_1d_acc) > 0:\n",
        "    print(\"NaN values found in features_1d_2 at indices:\", nan_indices_features_1d_acc)\n",
        "if len(nan_indices_Y) > 0:\n",
        "    print(\"NaN values found in Y at indices:\", nan_indices_Y)\n",
        "\n",
        "# Prepare Data\n",
        "testing_flag = 0\n",
        "data = Data(X_emg, X_acc, zc_features, ssc_features, features_1d_emg, features_1d_acc, Y)\n",
        "print(\"Pickling Data\")\n",
        "# dump data\n",
        "with open('k_fold_dishwashing.pickle', 'wb') as f:\n",
        "    pickle.dump(data,f,protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYxR1SUmuGsa"
      },
      "source": [
        "# K-fold Cross validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN2q4gXruOil",
        "outputId": "9d42f5a9-0b0f-4872-eec8-8aec454e21b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "188/188 [==============================] - 340s 2s/step - loss: 0.4935 - accuracy: 0.7426 - val_loss: 0.4070 - val_accuracy: 0.8056\n",
            "Epoch 2/60\n",
            "188/188 [==============================] - 318s 2s/step - loss: 0.3949 - accuracy: 0.8029 - val_loss: 0.3394 - val_accuracy: 0.8442\n",
            "Epoch 3/60\n",
            "188/188 [==============================] - 320s 2s/step - loss: 0.3662 - accuracy: 0.8206 - val_loss: 0.3841 - val_accuracy: 0.8204\n",
            "Epoch 4/60\n",
            "188/188 [==============================] - 326s 2s/step - loss: 0.3518 - accuracy: 0.8281 - val_loss: 0.3282 - val_accuracy: 0.8456\n",
            "Epoch 5/60\n",
            "188/188 [==============================] - 322s 2s/step - loss: 0.3375 - accuracy: 0.8341 - val_loss: 0.3652 - val_accuracy: 0.8276\n",
            "Epoch 6/60\n",
            "188/188 [==============================] - 304s 2s/step - loss: 0.3173 - accuracy: 0.8468 - val_loss: 0.3157 - val_accuracy: 0.8551\n",
            "Epoch 7/60\n",
            "188/188 [==============================] - 307s 2s/step - loss: 0.3037 - accuracy: 0.8538 - val_loss: 0.3456 - val_accuracy: 0.8407\n",
            "Epoch 8/60\n",
            "188/188 [==============================] - 312s 2s/step - loss: 0.2800 - accuracy: 0.8664 - val_loss: 0.2841 - val_accuracy: 0.8705\n",
            "Epoch 9/60\n",
            "188/188 [==============================] - 306s 2s/step - loss: 0.2719 - accuracy: 0.8716 - val_loss: 0.2934 - val_accuracy: 0.8665\n",
            "Epoch 10/60\n",
            "188/188 [==============================] - 303s 2s/step - loss: 0.2675 - accuracy: 0.8748 - val_loss: 0.2686 - val_accuracy: 0.8814\n",
            "Epoch 11/60\n",
            "188/188 [==============================] - 308s 2s/step - loss: 0.2497 - accuracy: 0.8830 - val_loss: 0.2970 - val_accuracy: 0.8675\n",
            "Epoch 12/60\n",
            "188/188 [==============================] - 307s 2s/step - loss: 0.2397 - accuracy: 0.8881 - val_loss: 0.2712 - val_accuracy: 0.8786\n",
            "Epoch 13/60\n",
            "188/188 [==============================] - 307s 2s/step - loss: 0.2330 - accuracy: 0.8916 - val_loss: 0.2320 - val_accuracy: 0.9007\n",
            "Epoch 14/60\n",
            "188/188 [==============================] - 303s 2s/step - loss: 0.2297 - accuracy: 0.8946 - val_loss: 0.3049 - val_accuracy: 0.8636\n",
            "Epoch 15/60\n",
            "188/188 [==============================] - 303s 2s/step - loss: 0.2222 - accuracy: 0.8957 - val_loss: 0.2346 - val_accuracy: 0.9000\n",
            "Epoch 16/60\n",
            "188/188 [==============================] - 304s 2s/step - loss: 0.2136 - accuracy: 0.9013 - val_loss: 0.2598 - val_accuracy: 0.8832\n",
            "Epoch 17/60\n",
            "188/188 [==============================] - 305s 2s/step - loss: 0.2143 - accuracy: 0.8991 - val_loss: 0.2117 - val_accuracy: 0.9070\n",
            "Epoch 18/60\n",
            "188/188 [==============================] - 301s 2s/step - loss: 0.2007 - accuracy: 0.9074 - val_loss: 0.1834 - val_accuracy: 0.9199\n",
            "Epoch 19/60\n",
            "188/188 [==============================] - 301s 2s/step - loss: 0.2026 - accuracy: 0.9061 - val_loss: 0.2368 - val_accuracy: 0.8988\n",
            "Epoch 20/60\n",
            "188/188 [==============================] - 309s 2s/step - loss: 0.1983 - accuracy: 0.9068 - val_loss: 0.2006 - val_accuracy: 0.9105\n",
            "Epoch 21/60\n",
            "188/188 [==============================] - 317s 2s/step - loss: 0.1906 - accuracy: 0.9107 - val_loss: 0.2049 - val_accuracy: 0.9102\n",
            "Epoch 22/60\n",
            "188/188 [==============================] - 306s 2s/step - loss: 0.1908 - accuracy: 0.9113 - val_loss: 0.1934 - val_accuracy: 0.9146\n",
            "Epoch 23/60\n",
            "188/188 [==============================] - 306s 2s/step - loss: 0.1906 - accuracy: 0.9092 - val_loss: 0.2284 - val_accuracy: 0.9012\n",
            "Epoch 24/60\n",
            "188/188 [==============================] - 302s 2s/step - loss: 0.1882 - accuracy: 0.9118 - val_loss: 0.2010 - val_accuracy: 0.9122\n",
            "Epoch 25/60\n",
            "188/188 [==============================] - 303s 2s/step - loss: 0.1699 - accuracy: 0.9211 - val_loss: 0.2410 - val_accuracy: 0.8976\n",
            "Epoch 26/60\n",
            "134/188 [====================>.........] - ETA: 1:18 - loss: 0.1673 - accuracy: 0.9223"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Load the pickle file\n",
        "with open('k_fold_dishwashing.pickle', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "\n",
        "# Convert one-hot encoded labels to binary labels\n",
        "binary_Y_train = np.argmax(data.Y_train, axis=1)\n",
        "\n",
        "# Define the number of folds\n",
        "k_folds = 5\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=k_folds)\n",
        "\n",
        "\n",
        "# Initialize lists to store evaluation metrics for each fold\n",
        "accuracy_scores = []\n",
        "loss_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "\n",
        "# Perform K-fold cross-validation\n",
        "for train_index, test_index in kf.split(data.X_emg_train, binary_Y_train):\n",
        "    X_emg_train_fold, X_emg_val_fold = data.X_emg_train[train_index], data.X_emg_train[test_index]\n",
        "    X_acc_train_fold, X_acc_val_fold = data.X_acc_train[train_index], data.X_acc_train[test_index]\n",
        "    zc_features_train_fold, zc_features_val_fold = data.zc_features_train[train_index], data.zc_features_train[test_index]\n",
        "    ssc_features_train_fold, ssc_features_val_fold = data.ssc_features_train[train_index], data.ssc_features_train[test_index]\n",
        "    features_1d_train_emg_fold, features_1d_val_emg_fold = data.features_1d_train_emg[train_index], data.features_1d_train_emg[test_index]\n",
        "    features_1d_train_acc_fold, features_1d_val_acc_fold = data.features_1d_train_acc[train_index], data.features_1d_train_acc[test_index]\n",
        "    Y_train_fold, Y_val_fold = data.Y_train[train_index], data.Y_train[test_index]\n",
        "\n",
        "    # Build and train the model\n",
        "    model = BILSTM()\n",
        "    model.train(X_emg_train_fold, X_acc_train_fold, zc_features_train_fold, ssc_features_train_fold, features_1d_train_emg_fold, features_1d_train_acc_fold, Y_train_fold,\n",
        "               X_emg_val_fold, X_acc_val_fold, zc_features_val_fold, ssc_features_val_fold, features_1d_val_emg_fold, features_1d_val_acc_fold, Y_val_fold)\n",
        "\n",
        "    model.evaluate(X_emg_val_fold, X_acc_val_fold, zc_features_val_fold, ssc_features_val_fold, features_1d_val_emg_fold, features_1d_val_acc_fold, Y_val_fold)\n",
        "    model.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMdUaOlBIkD7"
      },
      "source": [
        "# Individual Testing\n",
        "This section allows for training of single models.\n",
        "NOTE: Ensure Data split is correct. k-fold uses all data and splits on its own; otherwise must select adequate train/val split above in preprocessing section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udr4pYJlLRq3",
        "outputId": "9f502a86-3021-42fc-dd4a-fcf8d2fb6f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "190/190 [==============================] - 457s 2s/step - loss: 0.4890 - accuracy: 0.7342 - val_loss: 0.4840 - val_accuracy: 0.7498\n",
            "Epoch 2/60\n",
            "190/190 [==============================] - 424s 2s/step - loss: 0.3895 - accuracy: 0.8017 - val_loss: 0.3782 - val_accuracy: 0.8169\n",
            "Epoch 3/60\n",
            "190/190 [==============================] - 420s 2s/step - loss: 0.3589 - accuracy: 0.8188 - val_loss: 0.3772 - val_accuracy: 0.8189\n",
            "Epoch 4/60\n",
            "190/190 [==============================] - 406s 2s/step - loss: 0.3412 - accuracy: 0.8328 - val_loss: 0.3382 - val_accuracy: 0.8354\n",
            "Epoch 5/60\n",
            "190/190 [==============================] - 405s 2s/step - loss: 0.3180 - accuracy: 0.8469 - val_loss: 0.3137 - val_accuracy: 0.8523\n",
            "Epoch 6/60\n",
            "190/190 [==============================] - 410s 2s/step - loss: 0.3032 - accuracy: 0.8550 - val_loss: 0.3302 - val_accuracy: 0.8522\n",
            "Epoch 7/60\n",
            "190/190 [==============================] - 399s 2s/step - loss: 0.2877 - accuracy: 0.8650 - val_loss: 0.2832 - val_accuracy: 0.8710\n",
            "Epoch 8/60\n",
            "190/190 [==============================] - 402s 2s/step - loss: 0.2809 - accuracy: 0.8663 - val_loss: 0.2463 - val_accuracy: 0.8888\n",
            "Epoch 9/60\n",
            "190/190 [==============================] - 404s 2s/step - loss: 0.2578 - accuracy: 0.8793 - val_loss: 0.2590 - val_accuracy: 0.8816\n",
            "Epoch 10/60\n",
            "190/190 [==============================] - 404s 2s/step - loss: 0.2520 - accuracy: 0.8837 - val_loss: 0.2293 - val_accuracy: 0.9014\n",
            "Epoch 11/60\n",
            "190/190 [==============================] - 408s 2s/step - loss: 0.2445 - accuracy: 0.8870 - val_loss: 0.2334 - val_accuracy: 0.8960\n",
            "Epoch 12/60\n",
            "190/190 [==============================] - 406s 2s/step - loss: 0.2396 - accuracy: 0.8885 - val_loss: 0.2480 - val_accuracy: 0.8921\n",
            "Epoch 13/60\n",
            "190/190 [==============================] - 405s 2s/step - loss: 0.2263 - accuracy: 0.8945 - val_loss: 0.2194 - val_accuracy: 0.9046\n",
            "Epoch 14/60\n",
            " 65/190 [=========>....................] - ETA: 3:59 - loss: 0.2248 - accuracy: 0.8954"
          ]
        }
      ],
      "source": [
        "# Load the pickle file\n",
        "with open('0.8_train_val_split.pickle', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Build new model and run\n",
        "model = BILSTM()\n",
        "model.train(data.X_emg_train, data.X_acc_train, data.zc_features_train, data.ssc_features_train, data.features_1d_train_emg, data.features_1d_train_acc, data.Y_train,\n",
        "            data.X_emg_val, data.X_acc_val, data.zc_features_val, data.ssc_features_val, data.features_1d_val_emg, data.features_1d_val_acc, data.Y_val)\n",
        "model.evaluate(data.X_emg_val, data.X_acc_val, data.zc_features_val, data.ssc_features_val, data.features_1d_val_emg, data.features_1d_val_acc, data.Y_val)\n",
        "model.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leave-one-out\n",
        "This section was not utilized in final thesis results, however allowed one to perform a leave-one-out analysis. This analysis allowed one to have a trained model on 7/8 data collections. Then, the model is tested on the final 1 data collection."
      ],
      "metadata": {
        "id": "D5cxAk7JT36I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzVAS94QJbMz"
      },
      "source": [
        "Generate Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmhiVo1_IzQu",
        "outputId": "18a0ff20-abde-4fa1-85cf-07bdf2232906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepare Testing Paths:\n",
            "Processing Testing Files:\n",
            "/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/Testing/8_11_36_05.txt\n",
            "Generating Data...\n",
            "Concatenate Signal Data:\n"
          ]
        }
      ],
      "source": [
        "# Prepare paths to data access\n",
        "print(\"Prepare Testing Paths:\")\n",
        "paths = []\n",
        "repository = \"/content/drive/MyDrive/Thesis/Data/Segmented_Data/LSTM_GH_Data/Mindrove_Data/Thesis/Testing\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(repository):\n",
        "    # Get all .txt files in the directory\n",
        "    files = [os.path.join(repository, f) for f in os.listdir(repository) if f.endswith('.txt')]\n",
        "    paths.extend(files)\n",
        "else:\n",
        "    print(\"Directory does not exist:\", repository)\n",
        "\n",
        "# Preprocessing Signal File\n",
        "print(\"Processing Testing Files:\")\n",
        "signals = []\n",
        "count = 0\n",
        "for path in paths:\n",
        "    # if count >= 14:\n",
        "    #     break\n",
        "    print(path)\n",
        "    signal = Signal(path)\n",
        "    signal.to_numpy()\n",
        "    signal.generate()\n",
        "    signal.feature_generation()\n",
        "    signal.save_data()\n",
        "    signals.append(signal)\n",
        "    count += 1\n",
        "\n",
        "# Concatenate Train val Data\n",
        "print(\"Concatenate Signal Data:\")\n",
        "X_emg, X_acc, zc_features, ssc_features, features_1d_emg, features_1d_acc, Y = signals[0].load_data()\n",
        "for signal in signals[1:]:\n",
        "    x_emg, x_acc, zc_feature, ssc_feature, feature_1d_emg, feature_1d_acc, y = signal.load_data()\n",
        "    X_emg = np.append(X_emg, x_emg, axis=0)\n",
        "    X_acc = np.append(X_acc, x_acc, axis=0)\n",
        "    zc_features = np.append(zc_features, zc_feature, axis=0)\n",
        "    ssc_features = np.append(ssc_features, ssc_feature, axis=0)\n",
        "    features_1d_emg = np.append(features_1d_emg, feature_1d_emg, axis=0)\n",
        "    features_1d_acc = np.append(features_1d_acc, feature_1d_acc, axis=0)\n",
        "    Y = np.append(Y, y, axis=0)\n",
        "\n",
        "# Check for NaN values in concatenated arrays\n",
        "nan_indices_X_emg = np.argwhere(np.isnan(X_emg))\n",
        "nan_indices_X_acc = np.argwhere(np.isnan(X_acc))\n",
        "nan_indices_zc = np.argwhere(np.isnan(zc_features))\n",
        "nan_indices_ssc = np.argwhere(np.isnan(ssc_features))\n",
        "nan_indices_features_1d_emg = np.argwhere(np.isnan(features_1d_emg))\n",
        "nan_indices_features_1d_acc = np.argwhere(np.isnan(features_1d_acc))\n",
        "nan_indices_Y = np.argwhere(np.isnan(Y))\n",
        "if len(nan_indices_X_emg) > 0:\n",
        "    print(\"NaN values found in X at indices:\", nan_indices_X_emg)\n",
        "if len(nan_indices_X_acc) > 0:\n",
        "    print(\"NaN values found in X at indices:\", nan_indices_X_acc)\n",
        "if len(nan_indices_zc) > 0:\n",
        "    print(\"NaN values found in zc_features at indices:\", nan_indices_zc)\n",
        "if len(nan_indices_ssc) > 0:\n",
        "    print(\"NaN values found in ssc_features at indices:\", nan_indices_ssc)\n",
        "if len(nan_indices_features_1d_emg) > 0:\n",
        "    print(\"NaN values found in features_1d at indices:\", nan_indices_features_1d_emg)\n",
        "if len(nan_indices_features_1d_acc) > 0:\n",
        "    print(\"NaN values found in features_1d_2 at indices:\", nan_indices_features_1d_acc)\n",
        "if len(nan_indices_Y) > 0:\n",
        "    print(\"NaN values found in Y at indices:\", nan_indices_Y)\n",
        "\n",
        "# Prepare Data\n",
        "#Store Testing Data / X, zc_features, ssc_features, features_1d_emg, features_1d_acc\n",
        "filenames = ['Test_X_emg.pickle','Test_X_acc.pickle' 'Test_Y.pickle', 'Test_zc_features.pickle', 'Test_ssc_features.pickle', 'Test_features_1d_emg.pickle', 'Test_features_1d_acc.pickle']\n",
        "data_to_dump = [X_emg, X_acc, Y, zc_features, ssc_features, features_1d_emg, features_1d_acc]\n",
        "for filename, i in zip(filenames, data_to_dump):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(i, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-446dojIw6l"
      },
      "source": [
        "Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1dXB3XpIuGA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The below code was utilized in leave-one-out testing. It saves the evaluation results and predicted vs. actual labels to a\n",
        "text file for side-by-side comparison. This is useful when assessing why the model was not predicting as expected.\n",
        "\n",
        "\n",
        "# Import Pickled Data\n",
        "filenames = ['Test_X_emg.pickle', 'Test_X_acc.pickle', 'Test_Y.pickle', 'Test_features_1d_acc.pickle', 'Test_features_1d_emg.pickle', 'Test_ssc_features.pickle', 'Test_zc_features.pickle']\n",
        "variables = ['Test_X_emg', 'Test_Y', 'Test_features_1d_acc', 'Test_features_1d_emg', 'Test_ssc_features', 'Test_zc_features']\n",
        "for filename, var in zip(filenames, variables):\n",
        "    with open(filename, 'rb') as f:\n",
        "        globals()[var] = pickle.load(f) #Make global variables, maybe better way\n",
        "\n",
        "# Must normalize data\n",
        "normalized_X, normalized_zc_features, normalized_ssc_features, normalized_features_1d_emg, normalized_features_1d_acc = data.normalize(Test_X, Test_zc_features, Test_ssc_features, Test_features_1d_emg, Test_features_1d_acc)\n",
        "#Evaluate based on normalized data\n",
        "evaluation_results = model.evaluate(normalized_X, normalized_zc_features, normalized_ssc_features, normalized_features_1d_emg, normalized_features_1d_acc, Test_Y)\n",
        "# Predict based on normalized data\n",
        "predicted_labels = model.predict(normalized_X, normalized_zc_features, normalized_ssc_features, normalized_features_1d_emg, normalized_features_1d_acc)\n",
        "actual_labels = np.argmax(Test_Y, axis=1)\n",
        "# Save evaluation results and predicted vs. actual labels to a text file\n",
        "with open('evaluation_results_with_predictions.txt', 'w') as f:\n",
        "    print(\"Evaluation Results:\", file=f)\n",
        "    print(\"Accuracy:\", evaluation_results[0], file=f)\n",
        "    print(\"Loss:\", evaluation_results[1], file=f)\n",
        "    print(\"F1 Score:\", evaluation_results[2], file=f)\n",
        "    print(\"\\nPredicted vs. Actual Y-labels:\", file=f)\n",
        "    for i in range(len(predicted_labels)):\n",
        "        print(f\"Predicted: {predicted_labels[i]}, Actual: {actual_labels[i]}\", file=f)\n",
        "\n",
        "print(\"Evaluation results with predictions saved to 'evaluation_results_with_predictions.txt'\")\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}